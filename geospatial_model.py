# -*- coding: utf-8 -*-
"""Geospatial_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14uAlnrEG1YwqTb_zwaAShD87X3ecHue_
"""

from google.colab import drive
drive.mount('/content/drive')

import matplotlib.pyplot as plt
import pandas as pd
import geopandas as gpd
from scipy.stats import rankdata

# Load dataset
data = pd.read_csv('/content/drive/MyDrive/AI4C/Final_Project/northeast_invasive_insects.csv')

# Filter for Spotted Lanternfly
data = data[data['taxon_species_name'] == 'Lycorma delicatula']
data = data.dropna(subset=['time_observed_at'])

# Convert timestamps to datetime
data['time_observed_at'] = pd.to_datetime(data['time_observed_at'])

# Normalize timestamps for color mapping
data['time_seconds'] = (data['time_observed_at'] - data['time_observed_at'].min()).dt.total_seconds()
data['time_percentile'] = rankdata(data['time_seconds'], method='max') / len(data)

# Load states shapefile
shapefile_path = "/content/drive/MyDrive/AI4C/Final_Project/ne_10m_admin_1_states_provinces/ne_10m_admin_1_states_provinces.shp"
states = gpd.read_file(shapefile_path)

#print(states['name'])


northeast_states = ['New York', 'Vermont', 'New Hampshire', 'Maine','Massachusetts', 'Connecticut', 'Rhode Island', 'New Jersey', 'Pennsylvania', 'District of Columbia', ]

states = states[states['name'].isin(northeast_states)]

# Convert data to GeoDataFrame
data_gdf = gpd.GeoDataFrame(
    data,
    geometry=gpd.points_from_xy(data['longitude'], data['latitude']),
    crs="EPSG:4326"
)

# Plot
fig, ax = plt.subplots(figsize=(10, 8))

# Plot states
states.plot(ax=ax, color='none', edgecolor='black', linewidth=0.5, zorder=1)

# Plot sightings
sc = data_gdf.plot(
    ax=ax,
    column='time_percentile',
    cmap='coolwarm',
    alpha=0.7,
    markersize=10,
    legend=True,
    legend_kwds={'label': "Normalized Time (Older to Recent)"}
)

# Add titles and labels
for x, y, label in zip(states.geometry.centroid.x, states.geometry.centroid.y, states['name']):
  if -80 < x < -65.0 and 37.0 < y < 47.0:
    ax.text(x, y, label, fontsize=8, ha='center', zorder=3)
plt.xlim(-80.0, -65.0)  # Longitude range
plt.ylim(37.0, 47.0)    # Latitude range
plt.title("Time-Based Heatmap of Spotted Lanternfly Sightings by State")
plt.xlabel("Longitude")
plt.ylabel("Latitude")
plt.show()

import pandas as pd
import numpy as np
import geopandas as gpd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Input, TimeDistributed, Conv1D, MaxPooling1D, Flatten
from tensorflow.keras.optimizers import Adam
from scipy.stats import rankdata

# List of counties in the northeastern U.S.
northeast_counties = [
    "Fairfield", "Hartford", "Litchfield", "Middlesex", "New Haven", "New London", "Tolland", "Windham", # Connecticut
    "Androscoggin", "Aroostook", "Cumberland", "Franklin", "Hancock", "Kennebec", "Knox", "Lincoln",
    "Oxford", "Penobscot", "Piscataquis", "Sagadahoc", "Somerset", "Waldo", "Washington", "York", # Maine
    "Barnstable", "Berkshire", "Bristol", "Dukes", "Essex", "Franklin", "Hampden", "Hampshire", "Middlesex",
    "Nantucket", "Norfolk", "Plymouth", "Suffolk", "Worcester", # Massachusetts
    "Belknap", "Carroll", "Cheshire", "CoÃ¶s", "Grafton", "Hillsborough", "Merrimack", "Rockingham",
    "Strafford", "Sullivan", # New Hampshire
    "Atlantic", "Bergen", "Burlington", "Camden", "Cape May", "Cumberland", "Essex", "Gloucester",
    "Hudson", "Hunterdon", "Mercer", "Middlesex", "Monmouth", "Morris", "Ocean", "Passaic", "Salem",
    "Somerset", "Sussex", "Union", "Warren", # New Jersey
    "Albany", "Allegany", "Bronx", "Broome", "Cattaraugus", "Cayuga", "Chautauqua", "Chemung", "Chenango",
    "Clinton", "Columbia", "Cortland", "Delaware", "Dutchess", "Erie", "Essex", "Franklin", "Fulton",
    "Genesee", "Greene", "Hamilton", "Herkimer", "Jefferson", "Kings", "Lewis", "Livingston", "Madison",
    "Monroe", "Montgomery", "Nassau", "New York", "Niagara", "Oneida", "Onondaga", "Ontario", "Orange",
    "Orleans", "Oswego", "Otsego", "Putnam", "Queens", "Rensselaer", "Richmond", "Rockland", "Saratoga",
    "Schenectady", "Schoharie", "Schuyler", "Seneca", "St. Lawrence", "Steuben", "Suffolk", "Sullivan",
    "Tioga", "Tompkins", "Ulster", "Warren", "Washington", "Wayne", "Westchester", "Wyoming", "Yates", # New York
    "Adams", "Allegheny", "Armstrong", "Beaver", "Bedford", "Berks", "Blair", "Bradford", "Bucks", "Butler",
    "Cambria", "Cameron", "Carbon", "Centre", "Chester", "Clarion", "Clearfield", "Clinton", "Columbia",
    "Crawford", "Cumberland", "Dauphin", "Delaware", "Elk", "Erie", "Fayette", "Forest", "Franklin",
    "Fulton", "Greene", "Huntingdon", "Indiana", "Jefferson", "Juniata", "Lackawanna", "Lancaster",
    "Lawrence", "Lebanon", "Lehigh", "Luzerne", "Lycoming", "McKean", "Monroe", "Montgomery", "Montour",
    "Northampton", "Northumberland", "Perry", "Philadelphia", "Pike", "Potter", "Schuylkill", "Snyder",
    "Somerset", "Sullivan", "Susquehanna", "Tioga", "Union", "Venango", "Warren", "Washington", "Wayne",
    "Westmoreland", "Wyoming", "York", # Pennsylvania
    "Bristol", "Kent", "Newport", "Providence", "Washington", # Rhode Island
    "Addison", "Bennington", "Caledonia", "Chittenden", "Essex", "Franklin", "Grand Isle", "Lamoille",
    "Orange", "Orleans", "Rutland", "Washington", "Windham", "Windsor" # Vermont
]

# Load dataset
data = pd.read_csv('/content/drive/MyDrive/AI4C/Final_Project/northeast_invasive_insects.csv')

data = data[data['taxon_species_name'] == 'Lycorma delicatula']

# Convert time_observed_at to datetime and filter out rows without timestamps
data['time_observed_at'] = pd.to_datetime(data['time_observed_at'], errors='coerce')
data = data.dropna(subset=['time_observed_at']).reset_index(drop=True)

# Extract year, month, and day from time_observed_at
data['year'] = data['time_observed_at'].dt.year
data['month'] = data['time_observed_at'].dt.month
data['day'] = data['time_observed_at'].dt.day

# Calculate time in seconds and normalize
data['time_seconds'] = (data['time_observed_at'] - data['time_observed_at'].min()).dt.total_seconds()
data['time_percentile'] = rankdata(data['time_seconds'], method='max') / len(data)

# One-hot encode place_county_name and place_state_name
encoded_county = pd.get_dummies(data['place_county_name'], prefix='county')
encoded_state = pd.get_dummies(data['place_state_name'], prefix='state')

# Concatenate one-hot encoded features to the dataset
data = pd.concat([data, encoded_county, encoded_state], axis=1)

# Drop unnecessary columns for modeling
data.drop(['place_county_name', 'place_state_name', 'time_observed_at', 'time_seconds'], axis=1, inplace=True)

# Define features and target
features = ['latitude', 'longitude', 'year', 'month', 'day', 'time_percentile'] + list(encoded_county.columns) + list(encoded_state.columns)
X = data[features].values
y = data[['latitude', 'longitude']].values  # Predict future latitude and longitude

# Normalize features
scaler = MinMaxScaler()
X = scaler.fit_transform(X)

# Create sequences for LSTM
sequence_length = 12  # Use 12 time steps for each sequence
X_seq, y_seq = [], []
for i in range(len(X) - sequence_length):
    X_seq.append(X[i:i + sequence_length])
    y_seq.append(y[i + sequence_length])
X_seq, y_seq = np.array(X_seq), np.array(y_seq)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)

# Build LSTM model
model = Sequential([
    Input(shape=(sequence_length, X_seq.shape[2], 1)),  # Define input shape explicitly
    TimeDistributed(Conv1D(filters=32, kernel_size=3, activation='relu')),
    TimeDistributed(MaxPooling1D(pool_size=2)),
    TimeDistributed(Flatten()),
    LSTM(64, activation='relu', return_sequences=False),
    Dense(32, activation='relu'),
    Dense(2)  # Latitude and Longitude output
])

model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])

model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))

# Predict using the model
predictions = model.predict(X_test)

# Convert predictions to GeoDataFrame for visualization
predicted_df = pd.DataFrame(predictions, columns=['latitude', 'longitude'])

# Define the shapefile path
shapefile_path = "/content/drive/MyDrive/AI4C/Final_Project/ne_10m_admin_2_counties/ne_10m_admin_2_counties.shp"

# Load the shapefile
counties = gpd.read_file(shapefile_path)

# Check and assign CRS
predicted_gdf = gpd.GeoDataFrame(
    predicted_df,
    geometry=gpd.points_from_xy(predicted_df['longitude'], predicted_df['latitude']),
    crs="EPSG:4326"
)

counties = counties[counties['NAME'].isin(northeast_counties)]

# Plot predictions on filtered counties
fig, ax = plt.subplots(figsize=(10, 8))
counties.plot(ax=ax, color='white', edgecolor='black', zorder=1)
predicted_gdf.plot(ax=ax, color='red', alpha=0.05, zorder=2)

# Get the bounds of the spotted lanternfly sightings
min_lat, max_lat = data['latitude'].min(), data['latitude'].max()
min_lon, max_lon = data['longitude'].min(), data['longitude'].max()

for x, y, label in zip(counties.geometry.centroid.x, counties.geometry.centroid.y, counties['NAME']):
  if min_lon < x < max_lon and min_lat < y < max_lat:
    ax.text(x, y, label, fontsize=6, ha='center', zorder=3)
plt.xlim(min_lon, max_lon)  # Longitude range
plt.ylim(min_lat, max_lat)    # Latitude range

plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.title('Predicted Spread of Spotted Lanternfly by County')
plt.legend()
plt.show()