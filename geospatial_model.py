# -*- coding: utf-8 -*-
"""Geospatial_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14uAlnrEG1YwqTb_zwaAShD87X3ecHue_
"""

from google.colab import drive
drive.mount('/content/drive')

import matplotlib.pyplot as plt
import pandas as pd
import geopandas as gpd
from scipy.stats import rankdata

# Load dataset
data = pd.read_csv('/content/drive/MyDrive/AI4C/Final_Project/northeast_invasive_insects.csv')

# Filter for Species
data = data[data['taxon_species_name'] == 'Lymantria dispar']
data = data.dropna(subset=['time_observed_at'])

# Convert timestamps to datetime
data['time_observed_at'] = pd.to_datetime(data['time_observed_at'])

# Normalize timestamps for color mapping
data['time_seconds'] = (data['time_observed_at'] - data['time_observed_at'].min()).dt.total_seconds()
data['time_percentile'] = rankdata(data['time_seconds'], method='max') / len(data)

# Load states shapefile
shapefile_path = "/content/drive/MyDrive/AI4C/Final_Project/ne_10m_admin_1_states_provinces/ne_10m_admin_1_states_provinces.shp"
states = gpd.read_file(shapefile_path)

northeast_states = ['New York', 'Vermont', 'New Hampshire', 'Maine','Massachusetts', 'Connecticut', 'Rhode Island', 'New Jersey', 'Pennsylvania', 'District of Columbia', ]

states = states[states['name'].isin(northeast_states)]

# Convert data to GeoDataFrame
data_gdf = gpd.GeoDataFrame(
    data,
    geometry=gpd.points_from_xy(data['longitude'], data['latitude']),
    crs="EPSG:4326"
)

# Plot
fig, ax = plt.subplots(figsize=(10, 8))

# Plot states
states.plot(ax=ax, color='none', edgecolor='black', linewidth=0.5, zorder=1)

# Plot sightings
sc = data_gdf.plot(
    ax=ax,
    column='time_percentile',
    cmap='coolwarm',
    alpha=0.7,
    markersize=10,
    legend=True,
    legend_kwds={'label': "Normalized Time (Older to Recent)"}
)

# Add titles and labels
for x, y, label in zip(states.geometry.centroid.x, states.geometry.centroid.y, states['name']):
  if -80 < x < -65.0 and 37.0 < y < 47.0:
    ax.text(x, y, label, fontsize=8, ha='center', zorder=3)
plt.xlim(-80.0, -65.0)  # Longitude range
plt.ylim(37.0, 47.0)    # Latitude range
plt.title("Time-Based Heatmap of Spongy Moth by State")
plt.xlabel("Longitude")
plt.ylabel("Latitude")
plt.show()

import pandas as pd
import numpy as np
import geopandas as gpd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Input, TimeDistributed, Conv1D, MaxPooling1D, Flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from scipy.stats import rankdata
from sklearn.metrics import r2_score
from geopy.distance import geodesic

# List of counties in the northeastern U.S.
northeast_counties = [
    "Fairfield", "Hartford", "Litchfield", "Middlesex", "New Haven", "New London", "Tolland", "Windham", # Connecticut
    "Androscoggin", "Aroostook", "Cumberland", "Franklin", "Hancock", "Kennebec", "Knox", "Lincoln",
    "Oxford", "Penobscot", "Piscataquis", "Sagadahoc", "Somerset", "Waldo", "Washington", "York", # Maine
    "Barnstable", "Berkshire", "Bristol", "Dukes", "Essex", "Franklin", "Hampden", "Hampshire", "Middlesex",
    "Nantucket", "Norfolk", "Plymouth", "Suffolk", "Worcester", # Massachusetts
    "Belknap", "Carroll", "Cheshire", "Coös", "Grafton", "Hillsborough", "Merrimack", "Rockingham", 
    "Strafford", "Sullivan", # New Hampshire
    "Atlantic", "Bergen", "Burlington", "Camden", "Cape May", "Cumberland", "Essex", "Gloucester",
    "Hudson", "Hunterdon", "Mercer", "Middlesex", "Monmouth", "Morris", "Ocean", "Passaic", "Salem",
    "Somerset", "Sussex", "Union", "Warren", # New Jersey
    "Albany", "Allegany", "Bronx", "Broome", "Cattaraugus", "Cayuga", "Chautauqua", "Chemung", "Chenango",
    "Clinton", "Columbia", "Cortland", "Delaware", "Dutchess", "Erie", "Essex", "Franklin", "Fulton", 
    "Genesee", "Greene", "Hamilton", "Herkimer", "Jefferson", "Kings", "Lewis", "Livingston", "Madison",
    "Monroe", "Montgomery", "Nassau", "New York", "Niagara", "Oneida", "Onondaga", "Ontario", "Orange",
    "Orleans", "Oswego", "Otsego", "Putnam", "Queens", "Rensselaer", "Richmond", "Rockland", "Saratoga",
    "Schenectady", "Schoharie", "Schuyler", "Seneca", "St. Lawrence", "Steuben", "Suffolk", "Sullivan",
    "Tioga", "Tompkins", "Ulster", "Warren", "Washington", "Wayne", "Westchester", "Wyoming", "Yates", # New York
    "Adams", "Allegheny", "Armstrong", "Beaver", "Bedford", "Berks", "Blair", "Bradford", "Bucks", "Butler",
    "Cambria", "Cameron", "Carbon", "Centre", "Chester", "Clarion", "Clearfield", "Clinton", "Columbia", 
    "Crawford", "Cumberland", "Dauphin", "Delaware", "Elk", "Erie", "Fayette", "Forest", "Franklin", 
    "Fulton", "Greene", "Huntingdon", "Indiana", "Jefferson", "Juniata", "Lackawanna", "Lancaster", 
    "Lawrence", "Lebanon", "Lehigh", "Luzerne", "Lycoming", "McKean", "Monroe", "Montgomery", "Montour", 
    "Northampton", "Northumberland", "Perry", "Philadelphia", "Pike", "Potter", "Schuylkill", "Snyder", 
    "Somerset", "Sullivan", "Susquehanna", "Tioga", "Union", "Venango", "Warren", "Washington", "Wayne", 
    "Westmoreland", "Wyoming", "York", # Pennsylvania
    "Bristol", "Kent", "Newport", "Providence", "Washington", # Rhode Island
    "Addison", "Bennington", "Caledonia", "Chittenden", "Essex", "Franklin", "Grand Isle", "Lamoille",
    "Orange", "Orleans", "Rutland", "Washington", "Windham", "Windsor" # Vermont
]

# Load dataset
data = pd.read_csv('/content/drive/MyDrive/AI4C/Final_Project/northeast_invasive_insects.csv')

data = data[data['taxon_species_name'] == 'Lymantria dispar']

# Convert time_observed_at to datetime and filter out rows without timestamps
data['time_observed_at'] = pd.to_datetime(data['time_observed_at'], errors='coerce')
data = data.dropna(subset=['time_observed_at']).reset_index(drop=True)

# Extract year, month, and day from time_observed_at
data['year'] = data['time_observed_at'].dt.year
data['month'] = data['time_observed_at'].dt.month
data['day'] = data['time_observed_at'].dt.day

# Calculate time in seconds and normalize
data['time_seconds'] = (data['time_observed_at'] - data['time_observed_at'].min()).dt.total_seconds()
data['time_percentile'] = rankdata(data['time_seconds'], method='max') / len(data)

# One-hot encode place_county_name and place_state_name
encoded_county = pd.get_dummies(data['place_county_name'], prefix='county')
encoded_state = pd.get_dummies(data['place_state_name'], prefix='state')

# Concatenate one-hot encoded features to the dataset
data = pd.concat([data, encoded_county, encoded_state], axis=1)

# Drop unnecessary columns for modeling
data.drop(['place_county_name', 'place_state_name', 'time_observed_at', 'time_seconds'], axis=1, inplace=True)

# Define features and target
features = ['latitude', 'longitude', 'year', 'month', 'day', 'time_percentile'] + list(encoded_county.columns) + list(encoded_state.columns)
X = data[features].values
y = data[['latitude', 'longitude']].values  # Predict future latitude and longitude

# Normalize features
scaler = MinMaxScaler()
X = scaler.fit_transform(X)

# Create sequences for LSTM
sequence_length = 12  # Use 12 time steps for each sequence
X_seq, y_seq = [], []
for i in range(len(X) - sequence_length):
    X_seq.append(X[i:i + sequence_length])
    y_seq.append(y[i + sequence_length])
X_seq, y_seq = np.array(X_seq), np.array(y_seq)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)

# Build LSTM model
model = Sequential([
    Input(shape=(sequence_length, X_seq.shape[2], 1)),  # Define input shape explicitly
    TimeDistributed(Conv1D(filters=32, kernel_size=3, activation='relu')),
    TimeDistributed(MaxPooling1D(pool_size=2)),
    TimeDistributed(Flatten()),
    LSTM(64, activation='relu', return_sequences=False),
    Dense(32, activation='relu'),
    Dense(2)  # Latitude and Longitude output
])

model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])

# Define the ModelCheckpoint callback
checkpoint = ModelCheckpoint(
    filepath='best_model.keras',  # Filepath to save the model
    monitor='val_loss',        # Metric to monitor
    save_best_only=True,       # Save only the best weights
    mode='min',                # We want the minimum val_loss
    verbose=1                  # Show a message when saving occurs
)

# Define EarlyStopping
early_stopping = EarlyStopping(
    monitor='val_loss',  # Metric to monitor
    patience=1,          # Number of epochs with no improvement to stop
    restore_best_weights=True  # Restore the best weights after stopping
)

model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test), callbacks=[checkpoint, early_stopping])

model.load_weights('best_model.keras')

# Predict using the model
predictions = model.predict(X_test)

# Calculate MAE (already part of model metrics)
mae = np.mean(np.abs(y_test - predictions))
print("Mean Absolute Error (MAE):", mae)

# Calculate MSE
mse = np.mean(np.square(y_test - predictions))
print("Mean Squared Error (MSE):", mse)

# Calculate R-Squared
r2 = r2_score(y_test, predictions)
print("R-Squared (R²):", r2)

# Calculate Geographic Error
def haversine_error(y_true, y_pred):
    return [geodesic(y_true[i], y_pred[i]).km for i in range(len(y_true))]

geo_errors = haversine_error(y_test, predictions)
mean_geo_error = np.mean(geo_errors)
print("Mean Geographic Error (km):", mean_geo_error)


# Convert predictions to GeoDataFrame for visualization
predicted_df = pd.DataFrame(predictions, columns=['latitude', 'longitude'])

# Define the shapefile path
shapefile_path = "/content/drive/MyDrive/AI4C/Final_Project/ne_10m_admin_2_counties/ne_10m_admin_2_counties.shp"

# Load the shapefile
counties = gpd.read_file(shapefile_path)

# Check and assign CRS
predicted_gdf = gpd.GeoDataFrame(
    predicted_df,
    geometry=gpd.points_from_xy(predicted_df['longitude'], predicted_df['latitude']),
    crs="EPSG:4326"
)

counties = counties[counties['NAME'].isin(northeast_counties)]

# Plot predictions on filtered counties
fig, ax = plt.subplots(figsize=(10, 8))
counties.plot(ax=ax, color='white', edgecolor='black', zorder=1)
predicted_gdf.plot(ax=ax, color='red', alpha=0.1, zorder=2)

# Get the bounds of the spotted lanternfly sightings
min_lat, max_lat = predicted_gdf['latitude'].min(), predicted_gdf['latitude'].max()
min_lon, max_lon = predicted_gdf['longitude'].min(), predicted_gdf['longitude'].max()

for x, y, label in zip(counties.geometry.centroid.x, counties.geometry.centroid.y, counties['NAME']):
  if min_lon < x < max_lon and min_lat < y < max_lat:
    ax.text(x, y, label, fontsize=6, ha='center', zorder=3)

plt.xlim(min_lon, max_lon)  # Longitude range
plt.ylim(min_lat, max_lat)    # Latitude range

plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.title('Predicted Spread of Spongy Moth by County')
plt.legend()
plt.show()
